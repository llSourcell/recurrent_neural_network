{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "This is a pure numpy implementation of word generation using an RNN\n",
    "\n",
    "![alt text](http://corochann.com/wp-content/uploads/2017/05/text_sequence_predict.png \"Logo Title Text 1\")\n",
    "\n",
    "We're going to have our network learn how to predict the next words in a given paragraph. This will require a recurrent architecture since the network will have to remember a sequence of characters. The order matters. 1000 iterations and we'll have pronouncable english. The longer the training time the better. You can feed it any text sequence (words, python, HTML, etc.)\n",
    "\n",
    "## What is a Recurrent Network?\n",
    "\n",
    "Feedforward networks are great for learning a pattern between a set of inputs and outputs.\n",
    "![alt text](https://www.researchgate.net/profile/Sajad_Jafari3/publication/275334508/figure/fig1/AS:294618722783233@1447253985297/Fig-1-Schematic-of-the-multilayer-feed-forward-neural-network-proposed-to-model-the.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://s-media-cache-ak0.pinimg.com/236x/10/29/a9/1029a9a0534a768b4c4c2b5341bdd003--city-year-math-patterns.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://www.researchgate.net/profile/Hamza_Guellue/publication/223079746/figure/fig5/AS:305255788105731@1449790059371/Fig-5-Configuration-of-a-three-layered-feed-forward-neural-network.png\n",
    " \"Logo Title Text 1\")\n",
    "\n",
    "- temperature & location\n",
    "- height & weight\n",
    "- car speed and brand\n",
    "\n",
    "But what if the ordering of the data matters? \n",
    "\n",
    "![alt text](http://www.aboutcurrency.com/images/university/fxvideocourse/google_chart.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](http://news.mit.edu/sites/mit.edu.newsoffice/files/styles/news_article_image_top_slideshow/public/images/2016/vondrick-machine-learning-behavior-algorithm-mit-csail_0.jpg?itok=ruGmLJm2 \"Logo Title Text 1\")\n",
    "\n",
    "Alphabet, Lyrics of a song. These are stored using Conditional Memory. You can only access an element if you have access to the previous elements (like a linkedlist). \n",
    "\n",
    "Enter recurrent networks\n",
    "\n",
    "We feed the hidden state from the previous time step back into the the network at the next time step.\n",
    "\n",
    "![alt text](https://iamtrask.github.io/img/basic_recurrence_singleton.png \"Logo Title Text 1\")\n",
    "\n",
    "So instead of the data flow operation happening like this\n",
    "\n",
    "## input -> hidden -> output\n",
    "\n",
    "it happens like this\n",
    "\n",
    "## (input + prev_hidden) -> hidden -> output\n",
    "\n",
    "wait. Why not this?\n",
    "\n",
    "## (input + prev_input) -> hidden -> output\n",
    "\n",
    "Hidden recurrence learns what to remember whereas input recurrence is hard wired to just remember the immediately previous datapoint\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/ferret-rnn-151211092908/95/recurrent-neural-networks-part-1-theory-10-638.jpg?cb=1449826311 \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://www.mathworks.com/help/examples/nnet/win64/RefLayRecNetExample_01.png \"Logo Title Text 1\")\n",
    "\n",
    "RNN Formula\n",
    "![alt text](https://cdn-images-1.medium.com/max/1440/0*TUFnE2arCrMrCvxH.png \"Logo Title Text 1\")\n",
    "\n",
    "It basically says the current hidden state h(t) is a function f of the previous hidden state h(t-1) and the current input x(t). The theta are the parameters of the function f. The network typically learns to use h(t) as a kind of lossy summary of the task-relevant aspects of the past sequence of inputs up to t.\n",
    "\n",
    "Loss function\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1440/0*ZsEG2aWfgqtk9Qk5. \"Logo Title Text 1\")\n",
    "\n",
    "The total loss for a given sequence of x values paired with a sequence of y values would then be just the sum of the losses over all the time steps. For example, if L(t) is the negative log-likelihood\n",
    "of y (t) given x (1), . . . , x (t) , then sum them up you get the loss for the sequence \n",
    "\n",
    "\n",
    "## Our steps\n",
    "\n",
    "- Initialize weights randomly\n",
    "- Give the model a char pair (input char & target char. The target char is the char the network should guess, its the next char in our sequence)\n",
    "- Forward pass (We calculate the probability for every possible next char according to the state of the model, using the paramters)\n",
    "- Measure error (the distance between the previous probability and the target char)\n",
    "- We calculate gradients for each of our parameters to see their impact they have on the loss (backpropagation through time)\n",
    "- update all parameters in the direction via gradients that help to minimise the loss\n",
    "- Repeat! Until our loss is small AF\n",
    "\n",
    "## What are some use cases?\n",
    "\n",
    "- Time series prediction (weather forecasting, stock prices, traffic volume, etc. )\n",
    "- Sequential data generation (music, video, audio, etc.)\n",
    "\n",
    "## Other Examples\n",
    "\n",
    "-https://github.com/anujdutt9/RecurrentNeuralNetwork (binary addition)\n",
    "\n",
    "## What's next? \n",
    "\n",
    "1 LSTM Networks\n",
    "2 Bidirectional networks\n",
    "3 recursive networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code contains 4 parts\n",
    "* Load the trainning data\n",
    "  * encode char into vectors\n",
    "* Define the Recurrent Network\n",
    "* Define a loss function\n",
    "  * Forward pass\n",
    "  * Loss\n",
    "  * Backward pass\n",
    "* Define a function to create sentences from the model\n",
    "* Train the network\n",
    "  * Feed the network\n",
    "  * Calculate gradient and update the model parameters\n",
    "  * Output a text to see the progress of the training\n",
    " \n",
    "\n",
    "## Load the training data\n",
    "\n",
    "The network need a big txt file as an input.\n",
    "\n",
    "The content of the file will be used to train the network.\n",
    "\n",
    "I use Methamorphosis from Kafka (Public Domain). Because Kafka was one weird dude. I like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 137628 chars, 80 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('kafka.txt', 'r').read()\n",
    "\n",
    "chars = list(set(data)) \n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print ('data has %d chars, %d unique' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode/Decode char/vector\n",
    "\n",
    "Neural networks operate on vectors (a vector is an array of float)\n",
    "So we need a way to encode and decode a char as a vector.\n",
    "\n",
    "We'll count the number of unique chars (*vocab_size*). That will be the size of the vector. \n",
    "The vector contains only zero exept for the position of the char wherae the value is 1.\n",
    "\n",
    "#### So First let's calculate the *vocab_size*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'U': 0, 'j': 1, 'o': 2, '*': 3, '2': 4, 'B': 5, '!': 6, '4': 7, 'e': 8, 'M': 9, '8': 10, 'K': 11, ')': 12, 'T': 13, 'q': 14, 'I': 15, 'd': 16, 'm': 17, 'H': 18, 'p': 19, 'Q': 20, 'J': 21, '@': 22, '$': 23, '3': 24, 'N': 25, \"'\": 26, '/': 27, 'l': 28, 'n': 29, '\"': 30, 'r': 31, '?': 32, 'F': 33, '9': 34, '\\n': 35, '1': 36, 'C': 37, '0': 38, '%': 39, 'Y': 40, 'c': 41, 'W': 42, '-': 43, 'P': 44, ';': 45, 'x': 46, 'D': 47, '(': 48, 'E': 49, '5': 50, ' ': 51, 'ç': 52, ',': 53, 'L': 54, 'f': 55, 's': 56, 'g': 57, '.': 58, 'G': 59, '7': 60, 'v': 61, 'a': 62, '6': 63, 'A': 64, 'y': 65, 'w': 66, 'i': 67, 'S': 68, 'z': 69, 'u': 70, 'R': 71, 'k': 72, 'V': 73, 'X': 74, 't': 75, 'b': 76, 'h': 77, ':': 78, 'O': 79}\n",
      "{0: 'U', 1: 'j', 2: 'o', 3: '*', 4: '2', 5: 'B', 6: '!', 7: '4', 8: 'e', 9: 'M', 10: '8', 11: 'K', 12: ')', 13: 'T', 14: 'q', 15: 'I', 16: 'd', 17: 'm', 18: 'H', 19: 'p', 20: 'Q', 21: 'J', 22: '@', 23: '$', 24: '3', 25: 'N', 26: \"'\", 27: '/', 28: 'l', 29: 'n', 30: '\"', 31: 'r', 32: '?', 33: 'F', 34: '9', 35: '\\n', 36: '1', 37: 'C', 38: '0', 39: '%', 40: 'Y', 41: 'c', 42: 'W', 43: '-', 44: 'P', 45: ';', 46: 'x', 47: 'D', 48: '(', 49: 'E', 50: '5', 51: ' ', 52: 'ç', 53: ',', 54: 'L', 55: 'f', 56: 's', 57: 'g', 58: '.', 59: 'G', 60: '7', 61: 'v', 62: 'a', 63: '6', 64: 'A', 65: 'y', 66: 'w', 67: 'i', 68: 'S', 69: 'z', 70: 'u', 71: 'R', 72: 'k', 73: 'V', 74: 'X', 75: 't', 76: 'b', 77: 'h', 78: ':', 79: 'O'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "print (char_to_ix)\n",
    "print (ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we create 2 dictionary to encode and decode a char to an int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finaly we create a vector from a char like this:\n",
    "The dictionary defined above allosw us to create a vector of size 61 instead of 256.  \n",
    "Here and exemple of the char 'a'  \n",
    "The vector contains only zeros, except at position char_to_ix['a'] where we put a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print (vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the network\n",
    "\n",
    "The neural network is made of 3 layers:\n",
    "* an input layer\n",
    "* an hidden layer\n",
    "* an output layer\n",
    "\n",
    "All layers are fully connected to the next one: each node of a layer are conected to all nodes of the next layer.\n",
    "The hidden layer is connected to the output and to itself: the values from an iteration are used for the next one.\n",
    "\n",
    "To centralise values that matter for the training (_hyper parameters_) we also define the _sequence lenght_ and the _learning rate_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model parameters\n",
    "\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #input to hidden\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters are adjusted during the trainning.\n",
    "* _Wxh_ are parameters to connect a vector that contain one input to the hidden layer.\n",
    "* _Whh_ are parameters to connect the hidden layer to itself. This is the Key of the Rnn: Recursion is done by injecting the previous values from the output of the hidden state, to itself at the next iteration.\n",
    "* _Why_ are parameters to connect the hidden layer to the output\n",
    "* _bh_ contains the hidden bias\n",
    "* _by_ contains the output bias\n",
    "\n",
    "You'll see in the next section how theses parameters are used to create a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "\n",
    "The __loss__ is a key concept in all neural networks training. \n",
    "It is a value that describe how good is our model.  \n",
    "The smaller the loss, the better our model is.  \n",
    "(A good model is a model where the predicted output is close to the training output)\n",
    "  \n",
    "During the training phase we want to minimize the loss.\n",
    "\n",
    "The loss function calculates the loss but also the gradients (see backward pass):\n",
    "* It perform a forward pass: calculate the next char given a char from the training set.\n",
    "* It calculate the loss by comparing the predicted char to the target char. (The target char is the input following char in the tranning set)\n",
    "* It calculate the backward pass to calculate the gradients \n",
    "\n",
    "This function take as input:\n",
    "* a list of input char\n",
    "* a list of target char\n",
    "* and the previous hidden state\n",
    "\n",
    "This function outputs:\n",
    "* the loss\n",
    "* the gradient for each parameters between layers\n",
    "* the last hidden state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "The forward pass use the parameters of the model (Wxh, Whh, Why, bh, by) to calculate the next char given a char from the trainning set.\n",
    "\n",
    "xs[t] is the vector that encode the char at position t\n",
    "ps[t] is the probabilities for next char\n",
    "\n",
    "![alt text](https://deeplearning4j.org/img/recurrent_equation.png \"Logo Title Text 1\")\n",
    "\n",
    "```python\n",
    "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "```\n",
    "\n",
    "or is dirty pseudo code for each char\n",
    "```python\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh\n",
    "ys = hs*Why + by\n",
    "ps = normalized(ys)\n",
    "```\n",
    "\n",
    "### Backward pass\n",
    "\n",
    "The naive way to calculate all gradients would be to recalculate a loss for small variations for each parameters.\n",
    "This is possible but would be time consuming.\n",
    "There is a technics to calculates all the gradients for all the parameters at once: the backdrop propagation.  \n",
    "Gradients are calculated in the oposite order of the forward pass, using simple technics.  \n",
    "\n",
    "#### goal is to calculate gradients for the forward formula:\n",
    "```python\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh  \n",
    "ys = hs*Why + by\n",
    "```\n",
    "\n",
    "The loss for one datapoint\n",
    "![alt text](http://i.imgur.com/LlIMvek.png \"Logo Title Text 1\")\n",
    "\n",
    "How should the computed scores inside f change tto decrease the loss? We'll need to derive a gradient to figure that out.\n",
    "\n",
    "Since all output units contribute to the error of each hidden unit we sum up all the gradients calculated at each time step in the sequence and use it to update the parameters. So our parameter gradients becomes :\n",
    "\n",
    "![alt text](http://i.imgur.com/Ig9WGqP.png \"Logo Title Text 1\")\n",
    "\n",
    "Our first gradient of our loss. We'll backpropagate this via chain rule\n",
    "\n",
    "![alt text](http://i.imgur.com/SOJcNLg.png \"Logo Title Text 1\")\n",
    "\n",
    "The chain rule is a method for finding the derivative of composite functions, or functions that are made by combining one or more functions.\n",
    "\n",
    "![alt text](http://i.imgur.com/3Z2Rfdi.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](http://mathpullzone-8231.kxcdn.com/wp-content/uploads/thechainrule-image3.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://i0.wp.com/www.mathbootcamps.com/wp-content/uploads/thechainrule-image1.jpg?w=900 \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  inputs,targets are both list of integers.                                                                                                                                                   \n",
    "  hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
    "  returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
    "  \"\"\"\n",
    "  #store our inputs, hidden states, outputs, and probability values\n",
    "  xs, hs, ys, ps, = {}, {}, {}, {} #Empty dicts\n",
    "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
    "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
    "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
    "    # to calculate the hidden state at t = 0\n",
    "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
    "    # ps will take the ys and convert them to normalized probab for chars\n",
    "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
    "    # -1 as  a list index would wrap around to the final element\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  #init with previous hidden state\n",
    "    # Using \"=\" would create a reference, this creates a whole separate copy\n",
    "    # We don't want hs[-1] to automatically change if hprev is changed\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  #init loss as 0\n",
    "  loss = 0\n",
    "  # forward pass                                                                                                                                                                              \n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
    "    xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)                                                                                                                       \n",
    "  # backward pass: compute gradients going backwards    \n",
    "  #initalize vectors for gradient values for each set of weights \n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    #output probabilities\n",
    "    dy = np.copy(ps[t])\n",
    "    #derive our first gradient\n",
    "    dy[targets[t]] -= 1 # backprop into y  \n",
    "    #compute output gradient -  output times hidden states transpose\n",
    "    #When we apply the transpose weight matrix,  \n",
    "    #we can think intuitively of this as moving the error backward\n",
    "    #through the network, giving us some sort of measure of the error \n",
    "    #at the output of the lth layer. \n",
    "    #output gradient\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    #derivative of output bias\n",
    "    dby += dy\n",
    "    #backpropagate!\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "    dbh += dhraw #derivative of hidden bias\n",
    "    dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "    dhnext = np.dot(Whh.T, dhraw) \n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sentence from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " BVIU$2ç!x\n",
      "v0S0$TJCoPpnu\"q:çNtB)4Ve7Y7w$HGjçKRKx2LO/Q\n",
      "@:X*$6H8M%y)Sw5Qgx1;b@0dXsHkQRGdvnKhqRQHPL!XT\"JvnM1hR2NWNiySH)D;%KbNen5vYn..tdTBP%?WDwUblk:UEv5oUxR;B6wElpçyoRfrMY(aWVv7!-@'7:f0DI )y:SKe-K/Le3f/%A \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  sample a sequence of integers from the model                                                                                                                                                \n",
    "  h is memory state, seed_ix is seed letter for first time step   \n",
    "  n is how many characters to predict\n",
    "  \"\"\"\n",
    "  #create vector\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  #customize it for our seed char\n",
    "  x[seed_ix] = 1\n",
    "  #list to store generated chars\n",
    "  ixes = []\n",
    "  #for as many characters as we want to generate\n",
    "  for t in range(n):\n",
    "    #a hidden state at a given time step is a function \n",
    "    #of the input at the same time step modified by a weight matrix \n",
    "    #added to the hidden state of the previous time step \n",
    "    #multiplied by its own hidden state to hidden state matrix.\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    #compute output (unnormalised)\n",
    "    y = np.dot(Why, h) + by\n",
    "    ## probabilities for next chars\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    #pick one with the highest probability \n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    #create a vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    #customize it for the predicted char\n",
    "    x[ix] = 1\n",
    "    #add it to the list\n",
    "    ixes.append(ix)\n",
    "\n",
    "  txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "  print ('----\\n %s \\n----' % (txt, ))\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "sample(hprev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training\n",
    "\n",
    "This last part of the code is the main trainning loop:\n",
    "* Feed the network with portion of the file. Size of chunk is *seq_lengh*\n",
    "* Use the loss function to:\n",
    "  * Do forward pass to calculate all parameters for the model for a given input/output pairs\n",
    "  * Do backward pass to calculate all gradiens\n",
    "* Print a sentence from a random seed using the parameters of the network\n",
    "* Update the model using the Adaptative Gradien technique Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feed the loss function with inputs and targets\n",
    "\n",
    "We create two array of char from the data file,\n",
    "the targets one is shifted compare to the inputs one.\n",
    "\n",
    "For each char in the input array, the target array give the char that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [79, 29, 8, 51, 17, 2, 31, 29, 67, 29, 57, 53, 51, 66, 77, 8, 29, 51, 59, 31, 8, 57, 2, 31, 51]\n",
      "targets [29, 8, 51, 17, 2, 31, 29, 67, 29, 57, 53, 51, 66, 77, 8, 29, 51, 59, 31, 8, 57, 2, 31, 51, 68]\n"
     ]
    }
   ],
   "source": [
    "p=0  \n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print (\"inputs\", inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print (\"targets\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad to update the parameters\n",
    "\n",
    "This is a type of gradient descent strategy\n",
    "\n",
    "![alt text](http://www.logos.t.u-tokyo.ac.jp/~hassy/deep_learning/adagrad/adagrad2.png\n",
    " \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "\n",
    "step size = learning rate\n",
    "\n",
    "The easiest technics to update the parmeters of the model is this:\n",
    "\n",
    "```python\n",
    "param += dparam * step_size\n",
    "```\n",
    "Adagrad is a more efficient technique where the step_size are getting smaller during the training.\n",
    "\n",
    "It use a memory variable that grow over time:\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "```\n",
    "and use it to calculate the step_size:\n",
    "```python\n",
    "step_size = 1./np.sqrt(mem + 1e-8)\n",
    "```\n",
    "In short:\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update \n",
    "```\n",
    "\n",
    "### Smooth_loss\n",
    "\n",
    "Smooth_loss doesn't play any role in the training.\n",
    "It is just a low pass filtered version of the loss:\n",
    "```python\n",
    "smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "```\n",
    "\n",
    "It is a way to average the loss on over the last iterations to better track the progress\n",
    "\n",
    "\n",
    "### So finally\n",
    "Here the code of the main loop that does both trainning and generating text from times to times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 109.550676\n",
      "----\n",
      " zD:\n",
      "BB?LioI$PH9\n",
      "j,FLx3/l%y/v:WcW\n",
      "0 WQ *\"QESJ)8.C%50FQç UfM5jOx:EL2UTM%hb'Yv,JY'D$YIv3\n",
      "7*a,p9mPaR!n25(6zBV1yp-ç8PuDSPwwG2u6DB;*)f3On:8IDeM%h?HsO;ç@To?3W6ztuw0BT $F.$BCw3v5?YMemwMvl:l3uSE$(,nLnrs42v:3rE \n",
      "----\n",
      "iter 1000, loss: 86.014509\n",
      "----\n",
      " an be shis woit in enrhorkenondo toun ofhedyt hatun f.sbe iomeB wese tti he riwie ire ianfs trolce, Anod we de'lle I? whe thedt,rlly twounye st o wirnr tout toslJ9s wer Gn trogre nBfs th pete thedd ,  \n",
      "----\n",
      "iter 2000, loss: 69.025341\n",
      "----\n",
      " s hy fasherge us ally miwhd mes bifpacr;etot ser lamhent wishenthn thetomow she thim he he woeush hin, bocly onke skuri ter homk. Haseegtere. f wuded nowi\n",
      "t wathest hoslaid tos susaly her ere, ande sh \n",
      "----\n",
      "iter 3000, loss: 60.183069\n",
      "----\n",
      " mrot Gougg hes ho pasiry wouk ofetwerousw hi ast he sle hist se wBig wnresing, ifeth thed thoor theindstly faracis the cayrinded ow lllyting poothe sfpaledegod and bower'tere ghe fowm haldyes; pamerba \n",
      "----\n",
      "iter 4000, loss: 55.764195\n",
      "----\n",
      "  of, the bataing thollibug at fxos leo thid fos rt hime llangpeoop as thack pare the itha's her \"Cond at oret has \"f iss we forinn way hey dlyper, win thedch sland hiy nector bun nfithe hely tor tipra \n",
      "----\n",
      "iter 5000, loss: 57.952055\n",
      "----\n",
      " t o7 a. MFtsinpced 7adk doraniL\", wher of ineik ther thsting th wol*wo o the on. the. Whoube wi5om fok tore cowent\n",
      "igork ate rotilk buyte Soone7rd ankurly aple srtrouere bore (late Gr\n",
      "Sicherk tom thin \n",
      "----\n",
      "iter 6000, loss: 58.997796\n",
      "----\n",
      " ecd old hee hlely at he las cadces th wisn are blrow lerf wath tid eas to wyhe nocegin blach on zind as a buto eiste. Hic thact, that inund aouldt heit thimepa set ef to hal wand frabe motpsoeg \"*e If \n",
      "----\n",
      "iter 7000, loss: 54.690540\n",
      "----\n",
      " , at of theror rave tatwiprldy turk fmeort. The was ryor so gathiot atto hio sle way waiglong his the at and has inccath, in lot nid hat dorilk moting athiof to noren to tere to ald his hee ofor to to \n",
      "----\n",
      "iter 8000, loss: 51.582714\n",
      "----\n",
      " ould ceese imeed. Tho lome the war!y nome fof pit have tal Grow apderlly xerefore hith und\n",
      "ts anl be; as buto wome aad alf Gros - douke n on wime sha hqu. Aufts inst he ceviuld bod cleaksed and ap the \n",
      "----\n",
      "iter 9000, loss: 50.145588\n",
      "----\n",
      " omee leaslyou diksinf ass core an, ade othrop.e hif chere on foopererstow sourick?o? acoupme whabe kefringith himeecs, beyher coving ctout thery it the ladter.y do the prouk, ald nister buth abloll sh \n",
      "----\n",
      "iter 10000, loss: 49.681166\n",
      "----\n",
      "  juxdenithen cot thes wa. The her fald stound olsing pove, theg dithl to hin the that cout ver(did the fatarsing withacled alat thom. An abet. Tn the eiag ou dtroe coupruet all blounded in fllowirgep  \n",
      "----\n",
      "iter 11000, loss: 56.557325\n",
      "----\n",
      " d coutat a Thestpanery\n",
      "ORd ON.\n",
      "\n",
      "3elut at tiok R08the deawirt uld puld ace Fulaile of projint ibeisess is of taas wither Samson Yoteut coute Afuttintivh fase foyane belrlerth Greatt cainte faor anl che \n",
      "----\n",
      "iter 12000, loss: 53.277516\n",
      "----\n",
      " qatik owe s theter nading this has and wha io\"Do ble pectham ant tregr and cerouss perers yome noje certsis fonsthasay wask unkey nit oncerouya, orlen cherell all, to teropwing cont becthers ap lekwar \n",
      "----\n",
      "iter 13000, loss: 50.120523\n",
      "----\n",
      " ut. IFrled he hos moom:e lt aut ouy so head, this bo shen she hofwomens bicithou doon was athed. Heled if hear the atidser noiting ver't he him incore leell then ale ondy. 0jougice lat ancalif an he w \n",
      "----\n",
      "iter 14000, loss: 48.288686\n",
      "----\n",
      " thould and enchy the he door bulr cele.; Gregor, homer ant aplen, wat helf, thing wase he itser tish hass -tcerenibmed tre had the listle wither ofle dos himn, gher Gry.\n",
      "Sor whan, he rather lalk fiden \n",
      "----\n",
      "iter 15000, loss: 47.721110\n",
      "----\n",
      " to that her. The moniturdtingt's starnt he mad rids, hmor wheds und it out, she on rome has lont bew came to at tore the res. fook to hich stig that  ind mothe's ayder prog tienat bet wore ans s ondwl \n",
      "----\n",
      "iter 16000, loss: 50.771373\n",
      "----\n",
      " rithere ereentenbothert bela even/d angoors and. Sheste usceny andeysoIt; anstithed ander\n",
      "\n",
      ".E I/6NT.EIn Gregorgry\n",
      "he gotin, anthateny froth They hesme orley agombly. Shis kasting ox onsofnen mateet on \n",
      "----\n",
      "iter 17000, loss: 52.700228\n",
      "----\n",
      "  have as not ong- becthat the it becca, thas wat suopering pacly. Fokesaid he caid the way lestoughes as caas pise bass atevichiw it creched and aresenm tochis with bute notsly perallot his core of an \n",
      "----\n",
      "iter 18000, loss: 49.935517\n",
      "----\n",
      " n fallal witht but how was fiof hof all na that moow his bedlacgre frogicI loor ralid hikeding copelen he hims thound git!e't the cold thingenest onched a the eveut wi'f op rathwit, and wore exporm. i \n",
      "----\n",
      "iter 19000, loss: 47.847718\n",
      "----\n",
      " feffor's in seawer, he dedinl ivery othiou hade then where his bingod oper tieder ber creen, the waf of ould won now reater at in him besery. .I doresed of would he han, noakter in bucl in wher, it nr \n",
      "----\n",
      "iter 20000, loss: 47.028725\n",
      "----\n",
      " -telind, couth sther.\n",
      "\n",
      "- nort omshis quat the fpameed beefle on motise, this for hiss to leaped rust if doom we wall nosed it was of tiak. Shad. Squrest or, a fack'tle, and have of apreand wass plare  \n",
      "----\n",
      "iter 21000, loss: 46.893385\n",
      "----\n",
      " m, the noom cume-dem beed thepel asely beckly at anchapict lorm hin a been into inly sho lath had nome and nomet, she hily, chin's ut the comsing stish simsfaylloth as carctmow aed ast agoorh. Thre. S \n",
      "----\n",
      "iter 22000, loss: 52.865728\n",
      "----\n",
      " atioms entesar wO wetition andedse Nolp of a wis congien\n",
      "shas Gregor ofpener this opfiths as paiced abling but of,; naken and picadibuls the thase the Knot to for dlyobes to the fol to fede to critheu \n",
      "----\n",
      "iter 23000, loss: 50.448444\n",
      "----\n",
      " tiow this wace so provicuterbectrom doonser to she the freion hat there and coiss a? hellly than Grecerate heen, ind Gregor hrrite duwg mastroaid, siws styow asdigst wa deff Axtpa\" ytay, har he wromed \n",
      "----\n",
      "iter 24000, loss: 47.735511\n",
      "----\n",
      " y bet fot are varsing unled ancaen and bets and his delly tha forttsed on prethad fiather. sish, he dear \n",
      "ow the -s anle that his Sentr ghe\" wald thas sivering jut, eable; fulf would ever as fefmad si \n",
      "----\n",
      "iter 25000, loss: 46.160939\n",
      "----\n",
      "  tids, had at hey the mokeot tinned the should his mook was as took qundor was of youavingde no cluinden thoug tould thes pargor't dows it tele in tod nook onctay found ather ny had wouk's af bay\n",
      "ente \n",
      "----\n",
      "iter 26000, loss: 45.751943\n",
      "----\n",
      " ses, put ay quiwher in a rame alld warded reted with sothough batien. Greaspuster bumlingat to thever ores yosing it of heppnester incou sound no ame pell then the wi@r sunk of held thes ay On, stined \n",
      "----\n",
      "iter 27000, loss: 48.405661\n",
      "----\n",
      " w a hick the ge of to gelk \"samear jusally 3 jurgeded the allyenve iv thers, arceyon twa mere ald ase orright jomeingoumem. \n",
      "Puwg uroude to with mistiog chath eroth herpen\", inth - nhiot lither goware \n",
      "----\n",
      "iter 28000, loss: 50.404279\n",
      "----\n",
      " ) wornast with put boid to chy couttond fore, aided he shoirv hurd sousing the work cou conlemst berin, yow aryer in\"ul nomsted he evere bitiec er eest betund thons pace aver a, s cacist fout unsed ay \n",
      "----\n",
      "iter 29000, loss: 47.853306\n",
      "----\n",
      " surs the would otas fat, mpich as wis hears. wondon in his cant bed allarn\n",
      "shen he would bary hay have cher. Gregor, ome\", mes uld his cose sood, if to was hitt the of the han hew eecamy to ghe thomin \n",
      "----\n",
      "iter 30000, loss: 45.987097\n",
      "----\n",
      " iot nist buter thy wounn thu fude flaillt on betone, to even anftor wast Guthed, have he was sperserwory, Ghen nom birved -els welractary as had he hiver colly itted she and he betunfweven onedr Grey  \n",
      "----\n",
      "iter 31000, loss: 45.298709\n",
      "----\n",
      " ckioner\" sust he had if, clerter oneghan wall, sall nomu nes becald thaid to quillend gaarta he bey othisted en it ceacast makc uts soseryow aver, rutilr.y had get, to chielen wion inthey beed wis loo \n",
      "----\n",
      "iter 32000, loss: 45.132392\n",
      "----\n",
      " ooved of canite his ray ceporg orenon shes the keir. mowismed all in her farloor sherned wa gin shin in there on with, rost tomed noth plegorent was wayc, and surd jop, ared nit could sint womaly herb \n",
      "----\n",
      "iter 33000, loss: 50.507352\n",
      "----\n",
      " vibremer, loctly rood, thom ondens or the youct, the work not on i forlated\n",
      "ibe thoug- Greft larked pecher d sertepsly stenst anveng, a of warutede the grojit\n",
      "\n",
      " 5PUvawn goos bess losing cacade pistep, \n",
      "----\n",
      "iter 34000, loss: 48.516283\n",
      "----\n",
      " es\n",
      "ainted for beco -ss at ag busc sey topl his net as fres pry turnem damsed mate a reame furesse turttenr, whis waasing; haple therirms yow woulf. ailivenbiby haclussen. Whise ger, velve be to paiver \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 35000, loss: 46.077768\n",
      "----\n",
      " tst chysing fole the littoct soment thibup to semeffet poin night in a she, es to fory fhim, she some suthly the dike astelled cight ouply. No hould bf, ons at thousa a bece silades's mold as (ome Gre \n",
      "----\n",
      "iter 36000, loss: 44.698544\n",
      "----\n",
      " , as his come had athatulf what we sat Gregor's ckode, atwhely opel owed in it had sores, Ias caed ow the dithated work has bare, the che muse. OH he leckingef mitude he cooos as neidly bainst af the  \n",
      "----\n",
      "iter 37000, loss: 44.491169\n",
      "----\n",
      " faoply hin on thes, mebet as foth his tion dont ot. Gregor haved could hey and formev in. The lougise the rive that mush her his bacted heraring havss. \"Chis, bom(ly, him. Nones out, hin sandd and be  \n",
      "----\n",
      "iter 38000, loss: 46.718073\n",
      "----\n",
      " urmationedchack a macked ures\n",
      ", to and of listan olxinterong of\n",
      "they throw\n",
      "buid:ed, out a ply there,,, and ands if fathon he listore to woulroon snock loin the croure beytent and it bly, withis wroom  \n",
      "----\n",
      "iter 39000, loss: 48.870734\n",
      "----\n",
      " frole had fod, full thet for bath same af the frack parnt. Gregor's her'indre wnon molime stlete flow Gronize use to had be sut Voly Uound of quhaved gent of on heplecie bad it is earke had \"thand as  \n",
      "----\n",
      "iter 40000, loss: 46.534548\n",
      "----\n",
      " ick. Thead awind ad sto gnem this hander yok the chuen jut and hist hould as his stath he paincend himpowtilin it ore when to te mematheneven, as seearings a pron lythoizelf ha repent they dixt as on  \n",
      "----\n",
      "iter 41000, loss: 44.785009\n",
      "----\n",
      " nt the, his sbot'd a nowe din fron, sime rearrlent haved, nevor in ifainl het hey incest fustaraviry.\n",
      "\n",
      "Gregor to fiviomed. I dou couse wolleng und had of ander, as leedrormthed noth of in thoug arwhel \n",
      "----\n",
      "iter 42000, loss: 44.248435\n",
      "----\n",
      " ase fel, was coneeds feen, that was, wald doun the would thomed oo could they iturd Gregor wover a lod; to des was attary she wark, Gregor om.\" umdomen his ond sack beid intie clarnt sasiny forniom op \n",
      "----\n",
      "iter 43000, loss: 44.122411\n",
      "----\n",
      " guans repthtrovingtror hen ryousy thay worle sained at the would outh bading onen es for atyis. He was and wither that in a to tald soom thinyou hit his fan the says sisen the use than ovearly wamp, p \n",
      "----\n",
      "iter 44000, loss: 49.069696\n",
      "----\n",
      " ores pording ableanilgsane at ermated the papeyont of in\n",
      "and agent, mestard out, with tod beclecisis wementergast of crikiouss. suoveng the wlatwouts acrorenver the to compant on enfuth nos to ice sti \n",
      "----\n",
      "iter 45000, loss: 47.369180\n",
      "----\n",
      " k of I cemmusing his tard and - to caimpedgeen ef shis anpenfult carun ays ations, the frouts, ontither him I shispeng bey, ap had stoun af morencalers, spreed, card ixtreat, and all ceremem, eably. A \n",
      "----\n",
      "iter 46000, loss: 45.051772\n",
      "----\n",
      " y huy of no dorent if had noth sisbonbele Get that dor w5ut of his not sint agaple woway snagatly madle at to the coucter.  bray un, is to the coor wome inen, say beyomsich, has sthe mare alr. Than in \n",
      "----\n",
      "iter 47000, loss: 43.761550\n",
      "----\n",
      " it urou. his sardel Gregor trime had they his sarled for whoy his all, came do quitice fibered. \"Chert he's owhacs was bute the dimple over it same on had hel that was fore to gratint.\", yomst acef mo \n",
      "----\n",
      "iter 48000, loss: 43.616529\n",
      "----\n",
      " se, not? have nowlytished on ad ay here kering weveed stiat any, oot, an everghid her bumat is eforse wight?o, that had eventeg fer the maidarss ald whad on soub have to goasled and to shed bimed in h \n",
      "----\n",
      "iter 49000, loss: 45.567032\n",
      "----\n",
      " op to leentely deice apparainted, \"1EHid astmoulrou mackintly with conttrike the was reftritenbe, do entary od care the rouping yt ther the sove with unely, the oncedes lowed all herve cound af deepcs \n",
      "----\n",
      "iter 50000, loss: 47.827861\n",
      "----\n",
      " foim. Hingigh the susidask, sersery to sonid had tel hid dousing be on fronivispel If it, Ling and felund theic\" quef yoms, thatted fok peanithtobe of leen thath, Dovearly we sounde at if o mors trout \n",
      "----\n",
      "iter 51000, loss: 45.673247\n",
      "----\n",
      "  the from his out could, himpy for alchied wreace his sostion protet marning outorg entersos in hest to cemeds aghe wa drang, diss goor sperien wheme Lisimitutelf sull sewgately work, for his was fod  \n",
      "----\n",
      "iter 52000, loss: 43.988608\n",
      "----\n",
      " reved the what. Than cork fork was fory inpepettougrirt he stoup soon not to his ablot deemor the his ill beanit; way beg-ouldind. He naw in was they wnom of did to tad leght of the, now dople otto as \n",
      "----\n",
      "iter 53000, loss: 43.569954\n",
      "----\n",
      " move overrechen wein wey been and not fown dald aly Ald the, set would was and yes, hoimhaed at Grennint, sterces; hem hit, he way could wher he had fonth sillisprove the vert bey his fathould his who \n",
      "----\n",
      "iter 54000, loss: 43.421155\n",
      "----\n",
      " en or allong her prom's stetelthis packcupacurind thered athered hipiry she fnene. His sting to chase stly's wither, she fromproa to chow allathing han s dlat morion to maked had fowa weruwhellly cand \n",
      "----\n",
      "iter 55000, loss: 47.988379\n",
      "----\n",
      " y of machoup interele, porg-om beculd then for ferss:/\n",
      "1. Gut anleton the it reathely,\n",
      " the and the all frevtr cand desil acelp distich the se0mGp:ENEORK \"NTGrGL IAC.B1, Ive inlent with to day would s \n",
      "----\n",
      "iter 56000, loss: 46.585411\n",
      "----\n",
      " t to in bers fungsser ass be rosttinblyous the ficherks tiven thit then plaid. Gregor's male ter nests eraem groug he crarigutcrith in ontany now ug stirs I'ms, congseind - mate com afpen he lickork t \n",
      "----\n",
      "iter 57000, loss: 44.340063\n",
      "----\n",
      " had I with. \"He The vercomings and as to gek unxtre. sulxton that his sasister filling in it oos\"H, Projict asiand. Grener had a lottr weat eroumseen had dfoters, ed it\n",
      "surt; to mave. He dift. Whur; o \n",
      "----\n",
      "iter 58000, loss: 43.173700\n",
      "----\n",
      " lffor him at of rowe goss im his fss he had wamped who and, be be dight, maving aver. But her from, of whore's wese over fro mead a slish steakerdyse to said his por's a breght macted to not he aplit, \n",
      "----\n",
      "iter 59000, loss: 43.017017\n",
      "----\n",
      " en, so thather a coulf it his cond that lod even on his tifht for tions shough got the sthe thim haveed they impainither en, tha haw ir), at she fon the lawraking to he him on the krory with wardt, ma \n",
      "----\n",
      "iter 60000, loss: 44.806018\n",
      "----\n",
      " R Gutuen. His coull of had goth a room, wore ouping loor of itl it. We trowey his dicempareruse wat case tish that withings to ge from the out teet a uore apser tok. He twome from where Gregort with\n",
      "  \n",
      "----\n",
      "iter 61000, loss: 47.067705\n",
      "----\n",
      " s of a dotmen, arred gre tay mid way mut the in to in a Mrfinded to sthers. And stelly ontely on his s and flinsed for hatper bullaly and, frok the ort a whane houbly escath have and ortheed gatilly o \n",
      "----\n",
      "iter 62000, loss: 45.071046\n",
      "----\n",
      " it, himsonf hin in stike uffrent all the day him; did and proockiably lablt taltevan had sidey sut even - yoush apper that were not herde\"blat worr, whidl po thingoy, of the Yay of to dops acrekk youn \n",
      "----\n",
      "iter 63000, loss: 43.402165\n",
      "----\n",
      " et mat aser at hermatmet a hald, eveveryou loom it hended him the sicouscusetyase peveren the firsies, she waspe was ith uid to long they, was motill be as he, horkuster sided it the leaded sink fonth \n",
      "----\n",
      "iter 64000, loss: 43.030196\n",
      "----\n",
      " to ser, the compime gis moume aster streshising whalay listed he leant oxemsain. It gre on his sunt then dia cand un in at the dosabl was a fory ppeant- and they fuin. He stistion. That musiout no lid \n",
      "----\n",
      "iter 65000, loss: 42.879290\n",
      "----\n",
      "  room.\" The moly of he do in caattly a with becended the lead to came fawles ag his the spow vernielld whe towe diring be chither, 4ad do in. He mare, in all usfeed, file clesty a pack, Ald curpily di \n",
      "----\n",
      "iter 66000, loss: 47.159715\n",
      "----\n",
      " ol enatew aested to Preftongor obe dince tin prespicalled cat and und?\n",
      "\n",
      "The grook it. Fofare. But the prouping.\n",
      "\n",
      "They aby usen distermbuavt therec wout prothis wite way ted the wasniss bectreaking-tti \n",
      "----\n",
      "iter 67000, loss: 45.983574\n",
      "----\n",
      " mo haps, exentist to at thou for to Therk be he couldy alreve ole did if clet of strow yound, emplayed a fet forfel never reaker, cereser and vouse the withalild, (tmed a rits.\n",
      "Weching ound abod Onder \n",
      "----\n",
      "iter 68000, loss: 43.812055\n",
      "----\n",
      " intiont and oo would the on tomes shinging otrinibcel nathe beghiused out his ine the foll dound.. In hars dion no shawnet\". Gregilloting wat harlin of if itche and the inflly if abdyed though whem ti \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 69000, loss: 42.695995\n",
      "----\n",
      " vily as havley intr it peck by initer bush as ts wryomen the drour hur he) notsing, leld that sticuter to worre kel ovt\"\n",
      "\n",
      "Corecentce. The stattaber Gregor nothar saiding that be her as Shis mached, cl \n",
      "----\n",
      "iter 70000, loss: 42.498890\n",
      "----\n",
      " to the at the reight houcher and was no. Ithere room dlang that mimadeve stour ady\" poom. Angefor's woucrout what sectand, shat he lool and him. He was with hurr to anding. Gregor tell wepeghart had i \n",
      "----\n",
      "iter 71000, loss: 44.234636\n",
      "----\n",
      " r's. She direatilther. Somiegly with is the from his wankboz\n",
      "1 GUvensh, cremaine the tich his fion ws macker unde. s thrmstly. bisherlated bight working dey onveryisted room underig in the reace thet  \n",
      "----\n",
      "iter 72000, loss: 46.470344\n",
      "----\n",
      " oursing, frere in ale Gregor's tims op:using-y morenes the heldingelly sact to be counc, alls.  for the firsald all bected thinit he makt him lome the mother sulf and as so them all inttinger oufiorad \n",
      "----\n",
      "iter 73000, loss: 44.552705\n",
      "----\n",
      " im ifonire, !I UOF ID Iver the iver no ceacted to the dowat him thap sor sed was how to jod and foch of sI his sislicelers to the the hind out fromsid of he matthend strey seebousen. The - on had no h \n",
      "----\n",
      "iter 74000, loss: 42.912682\n",
      "----\n",
      " asling of it ti aly said, shat the sohe searnid rease Nose thoor's dosin. In the to shame; do bove will peanime the is susedels rewhim. He vertisilaseand grefen of han she Mr. The bed will isheen was  \n",
      "----\n",
      "iter 75000, loss: 42.626468\n",
      "----\n",
      " ocly us, howed. Sowarms his in, his entathe. Greger wour to har in even o been had ald if the dese as ore and seven warda fersiosed soupne. He was mutpen froug fration his was in her and bey bupfone w \n",
      "----\n",
      "iter 76000, loss: 42.443016\n",
      "----\n",
      " core since outher and saway the thand thisk dust whemsed gote ald way as ther\n",
      "to herden howe, and of asited everyt beaushy o jut thourd gavianencesly shing his prom streme his mayar isal ha perhasive  \n",
      "----\n",
      "iter 77000, loss: 46.451106\n",
      "----\n",
      " d to stleak all UCou with\n",
      "up on out in recte periss uncrourniend by the tipidenP\n",
      "Wou'rigsting of a lafil it wantistiod, upsanilyen as\n",
      "ver tways in the padk as caider.  hispions courint Youg\" this losi \n",
      "----\n",
      "iter 78000, loss: 45.511257\n",
      "----\n",
      "  of paage dosinglet shi- word, U Bugrome. The clariesede have dow yol monds brin ef the's fley \"y of noing whithed inw\" lenes if pare. .H smol of timpure it loway\n",
      "- in made to. The door of stilkss on  \n",
      "----\n",
      "iter 79000, loss: 43.355282\n",
      "----\n",
      " en but; he leserstertl his noted him would that he lase pedd wijuse in exciels henving,'s spiet anled he latyoted hood exyoding the stock could copthec Gregorver apor be oush out wher would fusire was \n",
      "----\n",
      "iter 80000, loss: 42.301773\n",
      "----\n",
      " s and seed to could allot hough sill to his of wore and a therlathin? -o bougs mush, Grether, anC. Mad a lork he ss that he was ros was povered the roor that sat he coupper that crainclions if have at \n",
      "----\n",
      "iter 81000, loss: 42.111052\n",
      "----\n",
      " ear, ande door wesper, soor ude, hungs an but it paighed look, to to vadyed beaine of tole agy mivisgoning, to cark, them carkly motcheap woth and herd, in they in him and toyid her steent they was hi \n",
      "----\n",
      "iter 82000, loss: 43.691691\n",
      "----\n",
      "  be so chist ge and bryted able wit a rlartong ablin in to commemslitt kithould wan conire's t.\n",
      "\n",
      "THe Projecuse of the (roat the dosest Greger a helriked, the works fusissind pupfice ailisted op fod im \n",
      "----\n",
      "iter 83000, loss: 45.941784\n",
      "----\n",
      " wrich. The donitrom\n",
      "it stow fonil of though the love etsible Aringor and porgovabecuwid of peement Projeppablight tif havings fation legent as his full one Grige; he moy, cale with to vould fath hompe \n",
      "----\n",
      "iter 84000, loss: 44.178448\n",
      "----\n",
      " ored.\n",
      "\n",
      "the Prove\",\n",
      "\"The the let, and Gregiticiss not io nowing held hat hass stile sellay equimble.\n",
      "\n",
      "AHl melthing Griggounde sarce he hes such, coceftiop fore, Gregoring hin fame nomer, had, shin had  \n",
      "----\n",
      "iter 85000, loss: 42.532874\n",
      "----\n",
      " he sleally and hou sllary. Thin the deceed into the from the who she pobe seapiry eeshreing, amu-n wither inicked of thigstentiotround cere the sfoor 3, a Gregiat , for aver with hagle now. Gregor of  \n",
      "----\n",
      "iter 86000, loss: 42.311722\n",
      "----\n",
      " rder and mald and the pospack todes at fron the was agairuw then shove to his noterly and, his, mor with She onslay it her flore his saving of herppses.  the was they grated and to Mry cereringe exent \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  # check \"How to feed the loss function to see how this part works\n",
    "  if p+seq_length+1 >= len(data) or n == 0:\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "    p = 0 # go from start of data                                                                                                                                                             \n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "  # sample from the model now and then                                                                                                                                                        \n",
    "  if n % 1000 == 0:\n",
    "    print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "    sample(hprev, inputs[0], 200)\n",
    "\n",
    "  # perform parameter update with Adagrad                                                                                                                                                     \n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "  p += seq_length # move data pointer                                                                                                                                                         \n",
    "  n += 1 # iteration counter    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
